# -*- coding: utf-8 -*-
"""Voice data with xai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g1FZGT1FGG3vEltlwOBFRP2jGtYjUi2V
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt
from lime import lime_tabular

# Load the dataset
url = '/content/drive/MyDrive/Parkinsson disease voice data set.csv'  # Replace with the actual URL or file path
df = pd.read_csv(url)

# Extract features and labels
X = df.drop(['name', 'status'], axis=1)  # Exclude 'name' column
y = df['status']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape features for CNN input (assuming one-dimensional input)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Build the neural network model with CNN layers
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model with a lower initial learning rate
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Learning rate scheduling
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

# Train the model without early stopping
history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2, callbacks=[reduce_lr])

# Plot training and validation accuracy over epochs
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Predict probabilities
y_probs = model.predict(X_test)

# Thresholding to get predicted classes (0 or 1)
y_pred = (y_probs > 0.5).astype(int)

import pandas as pd

# Load your dataset
data = pd.read_csv('/content/Parkinsson_disease.csv')  # Replace with the actual path to your dataset

# Filter instances with PD
pd_instances = data[data['status'] == 1]

# Filter instances without PD
healthy_instances = data[data['status'] == 0]

# Display counts
print("Number of instances with PD:", len(pd_instances))
print("Number of instances without PD:", len(healthy_instances))

accuracy = accuracy_score(y_test, y_pred)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

pip install shap

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import shap

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/Parkinsson disease voice data set.csv')

# Define the features and the target variable
X = data.iloc[:, 1:-1]
y = data.iloc[:, -1]

# Split the dataset into a training set and a testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Explain the model using SHAP
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize the explanations
shap.summary_plot(shap_values, X_test, plot_type="bar")